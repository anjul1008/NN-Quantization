{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f740a27",
   "metadata": {},
   "source": [
    "Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2f9f6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "589aa9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c75970c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "hidden_size = 32\n",
    "output_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "adc1abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "846fbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization Types\n",
    "# 1. Symmetric Quantization\n",
    "# 2. Asymmetric Quantization\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "    \n",
    "def clamp(x, min_val, max_val):\n",
    "    x[x<min_val] = min_val  \n",
    "    x[x>max_val] = max_val\n",
    "    return x\n",
    "\n",
    "def asymmetric_quantization(x, num_bits):\n",
    "    # min-max quantization\n",
    "    alpha = torch.max(torch.abs(x))\n",
    "    beta = 0\n",
    "    scale = (alpha - beta) / (2**num_bits - 1)\n",
    "    zero_point = -1 * torch.round(beta / scale)\n",
    "    quantized = clamp(torch.round(x / scale), -2**(num_bits-1), 2**(num_bits-1) - 1)\n",
    "    return quantized, scale, zero_point\n",
    "\n",
    "def asymmetric_dequantization(x, scale, zero_point):\n",
    "    return x * scale + zero_point\n",
    "\n",
    "def symmetric_quantization(x, num_bits):\n",
    "    alpha = torch.max(torch.abs(x))\n",
    "    scale = alpha / (2**(num_bits-1) - 1)\n",
    "    quantized = torch.round(x / scale)\n",
    "    return quantized, scale\n",
    "\n",
    "def symmetric_dequantization(x, scale):\n",
    "    return x * scale\n",
    "\n",
    "def quantize_percentile(x, num_bits, percentile=99.9):\n",
    "    alpha = np.percentile(x.numpy(), percentile)\n",
    "    scale = alpha / (2**(num_bits-1) - 1)\n",
    "    quantized = torch.round(x / scale)\n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_percentile(x, scale):\n",
    "    return x * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "40f89539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qunatized NN\n",
    "class QuantizedSimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(QuantizedSimpleNN, self).__init__()\n",
    "        self.qunat = torch.quantization.QuantStub()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.qunat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6a6a5461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SimpleNN(\n",
       "   (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "   (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "   (sigmoid): Sigmoid()\n",
       "   (relu): ReLU()\n",
       " ),\n",
       " QuantizedSimpleNN(\n",
       "   (qunat): QuantStub(\n",
       "     (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "   )\n",
       "   (fc1): Linear(\n",
       "     in_features=8, out_features=32, bias=True\n",
       "     (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "   )\n",
       "   (fc2): Linear(\n",
       "     in_features=32, out_features=16, bias=True\n",
       "     (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "   )\n",
       "   (sigmoid): Sigmoid()\n",
       "   (relu): ReLU()\n",
       "   (dequant): DeQuantStub()\n",
       " ))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qunat_model = QuantizedSimpleNN(input_size, hidden_size, output_size)\n",
    "qunat_model.load_state_dict(model.state_dict())\n",
    "qunat_model.eval()\n",
    "\n",
    "# Set quantization backend (required for conversion)\n",
    "# Use 'qnnpack' for ARM CPUs (like Apple Silicon) or 'fbgemm' for x86 CPUs\n",
    "torch.backends.quantized.engine = 'qnnpack'  # or 'fbgemm' for x86\n",
    "qunat_model.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')  # or 'fbgemm'\n",
    "qunat_model = torch.ao.quantization.prepare(qunat_model) # Insert observers\n",
    "\n",
    "model, qunat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "af731bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedSimpleNN(\n",
       "  (qunat): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=-2.006115674972534, max_val=2.4371373653411865)\n",
       "  )\n",
       "  (fc1): Linear(\n",
       "    in_features=8, out_features=32, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=-1.6454459428787231, max_val=1.8079413175582886)\n",
       "  )\n",
       "  (fc2): Linear(\n",
       "    in_features=32, out_features=16, bias=True\n",
       "    (activation_post_process): HistogramObserver(min_val=-0.6881318092346191, max_val=0.9596636891365051)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration step, estimate quantization param\n",
    "dummy_input = torch.randn(10, input_size)\n",
    "qunat_model(dummy_input)\n",
    "# model(dummy_input)\n",
    "qunat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4d815427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedSimpleNN(\n",
      "  (qunat): Quantize(scale=tensor([0.0174]), zero_point=tensor([115]), dtype=torch.quint8)\n",
      "  (fc1): QuantizedLinear(in_features=8, out_features=32, scale=0.013536081649363041, zero_point=122, qscheme=torch.per_tensor_affine)\n",
      "  (fc2): QuantizedLinear(in_features=32, out_features=16, scale=0.006458787713199854, zero_point=107, qscheme=torch.per_tensor_affine)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (relu): ReLU()\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "Size (KB): 5.416\n",
      "Size (KB): 4.962\n"
     ]
    }
   ],
   "source": [
    "qunat_model = torch.ao.quantization.convert(qunat_model)\n",
    "print(qunat_model)\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(qunat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e5fd04fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  97,    7,  115,  -96,  -45,  113, -122, -119],\n",
       "        [  21,  -88,  -36,    6,  -58,    0,   46,  -98],\n",
       "        [ 117,   55,   22, -105, -117,  -85,  -14,  -10],\n",
       "        [ -71,   73,   95,   80,    1,   86,  -86,   90],\n",
       "        [  56,  -49,  -31,   97, -105,  -84,  108,  -96],\n",
       "        [  12,   34,   -5,   17,  -87,   29, -128,   -9],\n",
       "        [  85,    5, -119,   40,   51,  -58,  -15,  -55],\n",
       "        [   0,   56,  -26,   15,  -92,   61,  -86,  -91],\n",
       "        [  -4,   73,   96,   40,   31, -113,  -44,  106],\n",
       "        [  38,  -71,  101,  -61,   89,  126, -102, -108],\n",
       "        [ -36,    2, -121,  101,  -68, -112,  -82,  -28],\n",
       "        [  41,   62,   13,  -82,   -5,  -17,  -55,   24],\n",
       "        [  25,  -23,  -12,    1,   76,  -15,  -18,  -47],\n",
       "        [ -99,  -98,  -61,   41, -118,  -87,    6,   44],\n",
       "        [ -67,  -14,  -40,  -69,  105,  -23,  -41,    3],\n",
       "        [ -18,   15,  -66, -107, -113,   26,    8,  -60],\n",
       "        [ -64,   54,   39,   62,  -90,   49,  112,  -54],\n",
       "        [ -37, -106,   25,  -47,  -78,  -92,   53,  -37],\n",
       "        [ -91,  -61,  112,   18,    6,  -40,  -61,  -99],\n",
       "        [  52,   88,   44,  -11, -124,  113,  -73,  -28],\n",
       "        [ -24,   51,   39,  -15,    3,  -92,   -6, -108],\n",
       "        [ -56, -120,  -78,  -92,   90,  -39,   29,  -15],\n",
       "        [ -91,  123,  121,   81,  -20,  -66,   72,  -83],\n",
       "        [ -92,  126,  -74,  -17, -108,  -92,  -14, -113],\n",
       "        [ 104, -102,   50,   20, -104,  117,   30,  -82],\n",
       "        [  -5,  -46,  -10,  101,   32,  110,   -7, -127],\n",
       "        [  86,  -52,   16,  -49,   -8,  -62,  -56,  105],\n",
       "        [ 125,  -32,   29,  -58,  126,    3,  -92,   36],\n",
       "        [ -45,  -57,    0,   49,   17,  120,  -75,  -60],\n",
       "        [ -70,   97,   74,  106,  -71, -102,   34, -117],\n",
       "        [  17,   33,  -53,  -10,   56,  -45,   -3,  -80],\n",
       "        [-109,  122,   87,  -41,  -73,  -37, -121,  -25]], dtype=torch.int8)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.int_repr(qunat_model.fc1.weight()) #data.dtype, model.fc1.weight.data.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k2_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
